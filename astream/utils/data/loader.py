import os
import json
import asyncio
from typing import List, Dict, Any, Optional
from astream.utils.logger import logger
from astream.utils.http.client import HttpClient
from astream.config.settings import settings


class DatasetLoader:
    """Gestionnaire du dataset d'URLs directes."""
    
    def __init__(self, http_client: HttpClient):
        self.http_client = http_client
        # Utiliser un chemin relatif qui fonctionne sur Windows et Linux
        self.dataset_path = os.path.join("data", "dataset.json")
        self.dataset = {"anime": []}
        self._anime_dict = {}  # Cache pour recherche O(1)
        
    async def initialize(self):
        """Initialise le dataset au d√©marrage."""
        try:
            # 1. V√©rifier si dataset local existe
            if os.path.exists(self.dataset_path):
                logger.log("DATASET", f"Dataset local trouv√©: {self.dataset_path}")
                self.dataset = self._load_local_dataset()
            else:
                # 2. Si pas de dataset local ET dataset activ√©
                if settings.DATASET_ENABLED:
                    logger.log("DATASET", "Aucun dataset local - T√©l√©chargement depuis GitHub")
                    await self._download_and_save_dataset()
                else:
                    logger.log("DATASET", "Dataset d√©sactiv√© - Utilisation dataset vide")
                    self.dataset = {"anime": []}
            
            # 3. Construire le cache de recherche
            self._build_search_cache()
            
            # 4. D√©marrer mise √† jour en arri√®re-plan si activ√©e
            if settings.DATASET_ENABLED and settings.DATASET_UPDATE_INTERVAL > 0:
                asyncio.create_task(self._periodic_update())
                
            logger.log("DATASET", f"Initialis√© avec {len(self.dataset.get('anime', []))} anime")
            
        except Exception as e:
            logger.error(f"DATASET: Erreur initialisation: {e}")
            self.dataset = {"anime": []}
            self._anime_dict = {}
    
    def _load_local_dataset(self) -> Dict[str, Any]:
        """Charge le dataset depuis le fichier local."""
        try:
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.debug(f"DATASET: Dataset local charg√© - {len(data.get('anime', []))} anime")
                return data
        except Exception as e:
            logger.warning(f"DATASET: Erreur lecture dataset local: {e}")
            return {"anime": []}
    
    async def _download_and_save_dataset(self):
        """T√©l√©charge et sauvegarde le dataset depuis GitHub."""
        try:
            if not settings.DATASET_URL:
                logger.warning("DATASET: DATASET_URL non configur√©e")
                return
                
            logger.log("DATASET", f"T√©l√©chargement depuis: {settings.DATASET_URL}")
            response = await self.http_client.get(settings.DATASET_URL)
            response.raise_for_status()
            
            remote_dataset = response.json()
            
            # Cr√©er le dossier /data si n√©cessaire
            os.makedirs(os.path.dirname(self.dataset_path), exist_ok=True)
            
            # Sauvegarder localement
            with open(self.dataset_path, 'w', encoding='utf-8') as f:
                json.dump(remote_dataset, f, ensure_ascii=False, indent=2)
                
            self.dataset = remote_dataset
            logger.log("SUCCESS", f"Dataset t√©l√©charg√© et sauv√© - {len(self.dataset.get('anime', []))} anime")
            
        except Exception as e:
            logger.error(f"DATASET: Erreur t√©l√©chargement: {e}")
            self.dataset = {"anime": []}
    
    def _build_search_cache(self):
        """Construit le cache de recherche pour performance O(1)."""
        self._anime_dict = {}
        
        for anime in self.dataset.get("anime", []):
            anime_slug = anime.get("slug")
            if anime_slug:
                if anime_slug not in self._anime_dict:
                    self._anime_dict[anime_slug] = {"streams": []}
                
                # Format avec URLs group√©es
                for stream in anime.get("streams", []):
                    season = stream.get("season")
                    episode = stream.get("episode") 
                    language = stream.get("language")
                    urls = stream.get("urls", [])
                        
                    if all([season is not None, episode is not None, language]):
                        for url in urls:
                            if url:
                                self._anime_dict[anime_slug]["streams"].append({
                                    "season": season,
                                    "episode": episode,
                                    "language": language,
                                    "url": url
                                })
        
        logger.debug(f"DATASET: Cache construit pour {len(self._anime_dict)} anime")
    
    async def get_streams(self, anime_slug: str, season: int, episode: int, language_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """R√©cup√®re les streams depuis le dataset pour un √©pisode donn√©."""
        try:
            if not self.dataset.get("anime") or anime_slug not in self._anime_dict:
                return []
            
            anime_data = self._anime_dict[anime_slug]
            matching_streams = []
            
            for stream in anime_data["streams"]:
                if stream["season"] == season and stream["episode"] == episode:
                    # Filtrer par langue si demand√©
                    if language_filter and language_filter != "Tout":
                        if language_filter == "VOSTFR" and stream["language"] != "VOSTFR":
                            continue
                        elif language_filter == "VF" and stream["language"] not in ["VF", "VF1", "VF2"]:
                            continue
                    
                    # Format compatible avec le reste du code
                    matching_streams.append({
                        "url": stream["url"],
                        "language": stream["language"]
                    })
            
            if matching_streams:
                logger.debug(f"DATASET: {len(matching_streams)} streams trouv√©s pour {anime_slug} S{season}E{episode}")
            
            return matching_streams
            
        except Exception as e:
            logger.error(f"DATASET: Erreur r√©cup√©ration streams {anime_slug}: {e}")
            return []
    
    async def _periodic_update(self):
        """Mise √† jour p√©riodique du dataset."""
        while True:
            try:
                # V√©rifier que l'intervalle est valide (>0)
                if settings.DATASET_UPDATE_INTERVAL <= 0:
                    logger.log("DATASET", "Mise √† jour p√©riodique d√©sactiv√©e (intervalle <= 0)")
                    break
                    
                await asyncio.sleep(settings.DATASET_UPDATE_INTERVAL)
                
                if not settings.DATASET_URL:
                    continue
                    
                logger.debug("üîÑ DATASET: V√©rification mise √† jour")
                
                # T√©l√©charger et appliquer nouvelle version
                response = await self.http_client.get(settings.DATASET_URL)
                response.raise_for_status()
                remote_dataset = response.json()
                
                # Sauvegarder nouvelle version
                with open(self.dataset_path, 'w', encoding='utf-8') as f:
                    json.dump(remote_dataset, f, ensure_ascii=False, indent=2)
                
                self.dataset = remote_dataset
                self._build_search_cache()
                
                logger.log("SUCCESS", f"Dataset mis √† jour - {len(self.dataset.get('anime', []))} anime")
                    
            except Exception as e:
                logger.warning(f"DATASET: Erreur mise √† jour p√©riodique: {e}")
    
    def reload_dataset(self):
        """Recharge le dataset depuis le fichier local (pour les scripts)."""
        try:
            self.dataset = self._load_local_dataset()
            self._build_search_cache()
            logger.log("SUCCESS", f"Dataset recharg√© - {len(self.dataset.get('anime', []))} anime")
        except Exception as e:
            logger.error(f"DATASET: Erreur rechargement: {e}")


# Instance globale
_dataset_loader: Optional[DatasetLoader] = None


def get_dataset_loader() -> Optional[DatasetLoader]:
    """R√©cup√®re l'instance globale du dataset loader."""
    return _dataset_loader


def set_dataset_loader(loader: DatasetLoader):
    """D√©finit l'instance globale du dataset loader."""
    global _dataset_loader
    _dataset_loader = loader